{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdGQ5Z3MPXK7C8Y8QfQ9lL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soosembly/ResearchPaper/blob/main/%08GoogLeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GoogLeNet은 주어진 하드웨어 자원을 최대한 효율적으로 이용하면서 학습 능력은 극대화할 수 있는 깊고 넓은 신경망.\n",
        "- 깊고 넓은 신경망을 위해 Inception 모듈을 추가함.\n",
        "- 인셉션 모듈 특징을 효율적으로 추출하기 위해 1x1, 3x3, 5x5의 합성곱 연산을 각각 수행\n",
        "- 3x3 최대 풀링은 입력과 출력의 높이와 너비가 같아야 하므로 풀링 연산에서는 드물게 '패딩을 추가해야함'\n",
        "- 결과적으로 GoogLeNet에 적용된 해결 방법은 sparse connectivity(희소 연결)임.\n",
        "- CNN은 합성곱, 풀링, 완전연결층들이 서로 dense(밀집, 정교하고 빽빽)하게 연결되어 있는데,\n",
        "- 희소 연결이라 함은 빽빽하게 연결된 신경망 대신 correlation이 높은 노드끼리만 연결하는 방법을 말함.\n",
        "- 이것으로 연산량이 적어지며 과적합도 해결할 수 있음\n",
        "- 대용량 데이터 학습을 해야할 때, 심층 신경망의 아키텍처에서 계층이 넓고(뉴런이 많고) 깊으면(계층이 많으면) 인식률은 좋아지지만, 과적합이나 vanishing gradient problem(기울기 소멸 문제)를 비롯한 학습 시간 지연과 연산 속도 등의 문제가 있는데, 특히 합성곱 신경망에서 이러한 문제들이 자주 나타남\n",
        "- 하지만 GoogLeNet(혹은 인셉션)으로 이러한 문제를 해결할 수 있다고 생각하면 됨."
      ],
      "metadata": {
        "id": "m-ZpbjUJcYT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 인셉션 모듈의 4가지 연산\n",
        "- 1x1 합성곱\n",
        "- 1x1 합성곱 + 3x3합성곱\n",
        "- 1x1 합성곱 + 5x5합성곱\n",
        "- 3x3 max pooling + 1x1합성곱(convolutional)"
      ],
      "metadata": {
        "id": "KSnM-tRCeA90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "id": "1H46820-HqCS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#01.Define Convolution Blocks"
      ],
      "metadata": {
        "id": "0UtyBGsTirjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional blocks\n",
        "def conv_1(in_dim, out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=1, stride=1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def conv_1_3(in_dim, mid_dim, out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, mid_dim, kernel_size=1, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(mid_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def conv_1_5(in_dim, mid_dim, out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, mid_dim, kernel_size=1, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(mid_dim, out_dim, kernel_size=5, stride=1, padding=2),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def max_3_1(in_dim, out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=1, stride=1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Vn4kKPH8ivgU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#02. Define Inception Module"
      ],
      "metadata": {
        "id": "kqC72EnEheo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eRJmqclicSBm"
      },
      "outputs": [],
      "source": [
        "class inception_module(nn.Module):\n",
        "    def __init__(self,in_dim,out_dim_1,mid_dim_3,out_dim_3,mid_dim_5,out_dim_5,pool_dim):\n",
        "        super(inception_module,self).__init__()\n",
        "        # 1x1 Convolution\n",
        "        self.conv_1 = conv_1(in_dim,out_dim_1)\n",
        "\n",
        "        # 1x1 Convolution -> 3x3 Convolution\n",
        "        self.conv_1_3 = conv_1_3(in_dim,mid_dim_3,out_dim_3)\n",
        "\n",
        "        # 1x1 Convolution -> 5x5 Convolution\n",
        "        self.conv_1_5 = conv_1_5(in_dim,mid_dim_5,out_dim_5)\n",
        "\n",
        "        # 3x3 MaxPooling -> 1x1 Convolution\n",
        "        self.max_3_1 = max_3_1(in_dim,pool_dim)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out_1 = self.conv_1(x)\n",
        "        out_2 = self.conv_1_3(x)\n",
        "        out_3 = self.conv_1_5(x)\n",
        "        out_4 = self.max_3_1(x)\n",
        "        # concat\n",
        "        output = torch.cat([out_1,out_2,out_3,out_4],1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#02. Define GoogLeNet"
      ],
      "metadata": {
        "id": "hT2SgzYKhnrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self, base_dim, num_classes=2):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        self.layer_1 = nn.Sequential(\n",
        "            nn.Conv2d(3, base_dim, kernel_size=7, stride=2, padding=3),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Conv2d(base_dim, base_dim * 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "        self.layer_2 = nn.Sequential(\n",
        "            inception_module(base_dim*3,64,96,128,16,32,32),\n",
        "            inception_module(base_dim*4,128,128,192,32,96,64),\n",
        "            nn.MaxPool2d(3,2,1),\n",
        "        )\n",
        "\n",
        "        self.layer_3 = nn.Sequential(\n",
        "            inception_module(480,192,96,208,16,48,64),\n",
        "            inception_module(512,160,112,224,24,64,64),\n",
        "            inception_module(512,128,128,256,24,64,64),\n",
        "            inception_module(512,112,144,288,32,64,64),\n",
        "            inception_module(528,256,160,320,32,128,128),\n",
        "            nn.MaxPool2d(3,2,1),\n",
        "        )\n",
        "\n",
        "        self.layer_4 = nn.Sequential(\n",
        "            inception_module(832,256,160,320,32,128,128),\n",
        "            inception_module(832,384,192,384,48,128,128),\n",
        "            nn.AvgPool2d(1,1),\n",
        "        )\n",
        "\n",
        "        # 평균 풀링과 드롭아웃 레이어\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "\n",
        "        # 완전 연결 층\n",
        "        self.fc_input_size = self.calculate_fc_input(base_dim)  # 여기에 저장\n",
        "        self.fc_layer = nn.Linear(self.fc_input_size, num_classes)\n",
        "\n",
        "\n",
        "    def calculate_fc_input(self, base_dim):\n",
        "        # 이 함수는 실제 네트워크의 출력 크기에 맞게 입력 차원을 계산해야 합니다.\n",
        "        mock_tensor = torch.zeros(1, 3, 32, 32)  # CIFAR10의 이미지 크기를 가정\n",
        "        mock_tensor = self.layer_1(mock_tensor)\n",
        "        mock_tensor = self.layer_2(mock_tensor)\n",
        "        mock_tensor = self.layer_3(mock_tensor)\n",
        "        mock_tensor = self.layer_4(mock_tensor)\n",
        "        return mock_tensor.size(1) * mock_tensor.size(2) * mock_tensor.size(3)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer_1(x)\n",
        "        out = self.layer_2(out)\n",
        "        out = self.layer_3(out)\n",
        "        out = self.layer_4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)  # Flatten\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc_layer(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "DWkDKF_uhsG9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "batch_size = 100\n",
        "learning_rate = 0.0002\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "AOZzADMYh1v7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드 및 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzi9I_fjMhF0",
        "outputId": "c5bbae8e-282d-45f9-b172-0c611bd56d31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29802814.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 인스턴스화 및 장치 설정\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = GoogLeNet(base_dim=64, num_classes=10).to(device)\n",
        "\n",
        "# 손실 함수 및 최적화 알고리즘 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "X0TZVkfiNDBb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#04.TRAIN"
      ],
      "metadata": {
        "id": "EhqBcS5Ph5-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # GPU로 이동\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # 매 100 미니배치마다 로그 출력\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_cq2we5h8OQ",
        "outputId": "d7aa5027-e4b8-4e4f-a786-600fb3c8f033"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 2.217\n",
            "Epoch 1, Batch 200, Loss: 2.035\n",
            "Epoch 1, Batch 300, Loss: 1.950\n",
            "Epoch 1, Batch 400, Loss: 1.887\n",
            "Epoch 1, Batch 500, Loss: 1.826\n",
            "Epoch 2, Batch 100, Loss: 1.763\n",
            "Epoch 2, Batch 200, Loss: 1.706\n",
            "Epoch 2, Batch 300, Loss: 1.656\n",
            "Epoch 2, Batch 400, Loss: 1.610\n",
            "Epoch 2, Batch 500, Loss: 1.588\n",
            "Epoch 3, Batch 100, Loss: 1.533\n",
            "Epoch 3, Batch 200, Loss: 1.523\n",
            "Epoch 3, Batch 300, Loss: 1.458\n",
            "Epoch 3, Batch 400, Loss: 1.456\n",
            "Epoch 3, Batch 500, Loss: 1.448\n",
            "Epoch 4, Batch 100, Loss: 1.379\n",
            "Epoch 4, Batch 200, Loss: 1.336\n",
            "Epoch 4, Batch 300, Loss: 1.324\n",
            "Epoch 4, Batch 400, Loss: 1.310\n",
            "Epoch 4, Batch 500, Loss: 1.307\n",
            "Epoch 5, Batch 100, Loss: 1.247\n",
            "Epoch 5, Batch 200, Loss: 1.238\n",
            "Epoch 5, Batch 300, Loss: 1.224\n",
            "Epoch 5, Batch 400, Loss: 1.201\n",
            "Epoch 5, Batch 500, Loss: 1.178\n",
            "Epoch 6, Batch 100, Loss: 1.134\n",
            "Epoch 6, Batch 200, Loss: 1.118\n",
            "Epoch 6, Batch 300, Loss: 1.141\n",
            "Epoch 6, Batch 400, Loss: 1.111\n",
            "Epoch 6, Batch 500, Loss: 1.086\n",
            "Epoch 7, Batch 100, Loss: 1.034\n",
            "Epoch 7, Batch 200, Loss: 1.013\n",
            "Epoch 7, Batch 300, Loss: 1.037\n",
            "Epoch 7, Batch 400, Loss: 1.020\n",
            "Epoch 7, Batch 500, Loss: 0.994\n",
            "Epoch 8, Batch 100, Loss: 0.934\n",
            "Epoch 8, Batch 200, Loss: 0.899\n",
            "Epoch 8, Batch 300, Loss: 0.916\n",
            "Epoch 8, Batch 400, Loss: 0.932\n",
            "Epoch 8, Batch 500, Loss: 0.907\n",
            "Epoch 9, Batch 100, Loss: 0.825\n",
            "Epoch 9, Batch 200, Loss: 0.835\n",
            "Epoch 9, Batch 300, Loss: 0.846\n",
            "Epoch 9, Batch 400, Loss: 0.828\n",
            "Epoch 9, Batch 500, Loss: 0.825\n",
            "Epoch 10, Batch 100, Loss: 0.766\n",
            "Epoch 10, Batch 200, Loss: 0.784\n",
            "Epoch 10, Batch 300, Loss: 0.754\n",
            "Epoch 10, Batch 400, Loss: 0.766\n",
            "Epoch 10, Batch 500, Loss: 0.765\n",
            "Epoch 11, Batch 100, Loss: 0.678\n",
            "Epoch 11, Batch 200, Loss: 0.686\n",
            "Epoch 11, Batch 300, Loss: 0.683\n",
            "Epoch 11, Batch 400, Loss: 0.700\n",
            "Epoch 11, Batch 500, Loss: 0.691\n",
            "Epoch 12, Batch 100, Loss: 0.606\n",
            "Epoch 12, Batch 200, Loss: 0.617\n",
            "Epoch 12, Batch 300, Loss: 0.633\n",
            "Epoch 12, Batch 400, Loss: 0.625\n",
            "Epoch 12, Batch 500, Loss: 0.658\n",
            "Epoch 13, Batch 100, Loss: 0.541\n",
            "Epoch 13, Batch 200, Loss: 0.544\n",
            "Epoch 13, Batch 300, Loss: 0.548\n",
            "Epoch 13, Batch 400, Loss: 0.584\n",
            "Epoch 13, Batch 500, Loss: 0.556\n",
            "Epoch 14, Batch 100, Loss: 0.459\n",
            "Epoch 14, Batch 200, Loss: 0.480\n",
            "Epoch 14, Batch 300, Loss: 0.483\n",
            "Epoch 14, Batch 400, Loss: 0.507\n",
            "Epoch 14, Batch 500, Loss: 0.511\n",
            "Epoch 15, Batch 100, Loss: 0.425\n",
            "Epoch 15, Batch 200, Loss: 0.433\n",
            "Epoch 15, Batch 300, Loss: 0.452\n",
            "Epoch 15, Batch 400, Loss: 0.434\n",
            "Epoch 15, Batch 500, Loss: 0.449\n",
            "Epoch 16, Batch 100, Loss: 0.357\n",
            "Epoch 16, Batch 200, Loss: 0.363\n",
            "Epoch 16, Batch 300, Loss: 0.394\n",
            "Epoch 16, Batch 400, Loss: 0.396\n",
            "Epoch 16, Batch 500, Loss: 0.391\n",
            "Epoch 17, Batch 100, Loss: 0.313\n",
            "Epoch 17, Batch 200, Loss: 0.332\n",
            "Epoch 17, Batch 300, Loss: 0.335\n",
            "Epoch 17, Batch 400, Loss: 0.332\n",
            "Epoch 17, Batch 500, Loss: 0.356\n",
            "Epoch 18, Batch 100, Loss: 0.275\n",
            "Epoch 18, Batch 200, Loss: 0.293\n",
            "Epoch 18, Batch 300, Loss: 0.295\n",
            "Epoch 18, Batch 400, Loss: 0.305\n",
            "Epoch 18, Batch 500, Loss: 0.323\n",
            "Epoch 19, Batch 100, Loss: 0.251\n",
            "Epoch 19, Batch 200, Loss: 0.246\n",
            "Epoch 19, Batch 300, Loss: 0.275\n",
            "Epoch 19, Batch 400, Loss: 0.272\n",
            "Epoch 19, Batch 500, Loss: 0.286\n",
            "Epoch 20, Batch 100, Loss: 0.207\n",
            "Epoch 20, Batch 200, Loss: 0.224\n",
            "Epoch 20, Batch 300, Loss: 0.245\n",
            "Epoch 20, Batch 400, Loss: 0.232\n",
            "Epoch 20, Batch 500, Loss: 0.257\n",
            "Epoch 21, Batch 100, Loss: 0.181\n",
            "Epoch 21, Batch 200, Loss: 0.208\n",
            "Epoch 21, Batch 300, Loss: 0.194\n",
            "Epoch 21, Batch 400, Loss: 0.220\n",
            "Epoch 21, Batch 500, Loss: 0.223\n",
            "Epoch 22, Batch 100, Loss: 0.163\n",
            "Epoch 22, Batch 200, Loss: 0.188\n",
            "Epoch 22, Batch 300, Loss: 0.189\n",
            "Epoch 22, Batch 400, Loss: 0.167\n",
            "Epoch 22, Batch 500, Loss: 0.206\n",
            "Epoch 23, Batch 100, Loss: 0.155\n",
            "Epoch 23, Batch 200, Loss: 0.185\n",
            "Epoch 23, Batch 300, Loss: 0.160\n",
            "Epoch 23, Batch 400, Loss: 0.173\n",
            "Epoch 23, Batch 500, Loss: 0.170\n",
            "Epoch 24, Batch 100, Loss: 0.155\n",
            "Epoch 24, Batch 200, Loss: 0.143\n",
            "Epoch 24, Batch 300, Loss: 0.139\n",
            "Epoch 24, Batch 400, Loss: 0.174\n",
            "Epoch 24, Batch 500, Loss: 0.153\n",
            "Epoch 25, Batch 100, Loss: 0.133\n",
            "Epoch 25, Batch 200, Loss: 0.134\n",
            "Epoch 25, Batch 300, Loss: 0.154\n",
            "Epoch 25, Batch 400, Loss: 0.151\n",
            "Epoch 25, Batch 500, Loss: 0.145\n",
            "Epoch 26, Batch 100, Loss: 0.129\n",
            "Epoch 26, Batch 200, Loss: 0.132\n",
            "Epoch 26, Batch 300, Loss: 0.131\n",
            "Epoch 26, Batch 400, Loss: 0.137\n",
            "Epoch 26, Batch 500, Loss: 0.135\n",
            "Epoch 27, Batch 100, Loss: 0.115\n",
            "Epoch 27, Batch 200, Loss: 0.113\n",
            "Epoch 27, Batch 300, Loss: 0.109\n",
            "Epoch 27, Batch 400, Loss: 0.111\n",
            "Epoch 27, Batch 500, Loss: 0.140\n",
            "Epoch 28, Batch 100, Loss: 0.116\n",
            "Epoch 28, Batch 200, Loss: 0.122\n",
            "Epoch 28, Batch 300, Loss: 0.111\n",
            "Epoch 28, Batch 400, Loss: 0.137\n",
            "Epoch 28, Batch 500, Loss: 0.145\n",
            "Epoch 29, Batch 100, Loss: 0.110\n",
            "Epoch 29, Batch 200, Loss: 0.095\n",
            "Epoch 29, Batch 300, Loss: 0.100\n",
            "Epoch 29, Batch 400, Loss: 0.140\n",
            "Epoch 29, Batch 500, Loss: 0.124\n",
            "Epoch 30, Batch 100, Loss: 0.067\n",
            "Epoch 30, Batch 200, Loss: 0.103\n",
            "Epoch 30, Batch 300, Loss: 0.129\n",
            "Epoch 30, Batch 400, Loss: 0.123\n",
            "Epoch 30, Batch 500, Loss: 0.109\n",
            "Epoch 31, Batch 100, Loss: 0.071\n",
            "Epoch 31, Batch 200, Loss: 0.103\n",
            "Epoch 31, Batch 300, Loss: 0.106\n",
            "Epoch 31, Batch 400, Loss: 0.106\n",
            "Epoch 31, Batch 500, Loss: 0.126\n",
            "Epoch 32, Batch 100, Loss: 0.079\n",
            "Epoch 32, Batch 200, Loss: 0.097\n",
            "Epoch 32, Batch 300, Loss: 0.122\n",
            "Epoch 32, Batch 400, Loss: 0.103\n",
            "Epoch 32, Batch 500, Loss: 0.095\n",
            "Epoch 33, Batch 100, Loss: 0.077\n",
            "Epoch 33, Batch 200, Loss: 0.091\n",
            "Epoch 33, Batch 300, Loss: 0.089\n",
            "Epoch 33, Batch 400, Loss: 0.096\n",
            "Epoch 33, Batch 500, Loss: 0.127\n",
            "Epoch 34, Batch 100, Loss: 0.068\n",
            "Epoch 34, Batch 200, Loss: 0.072\n",
            "Epoch 34, Batch 300, Loss: 0.096\n",
            "Epoch 34, Batch 400, Loss: 0.084\n",
            "Epoch 34, Batch 500, Loss: 0.100\n",
            "Epoch 35, Batch 100, Loss: 0.081\n",
            "Epoch 35, Batch 200, Loss: 0.086\n",
            "Epoch 35, Batch 300, Loss: 0.086\n",
            "Epoch 35, Batch 400, Loss: 0.104\n",
            "Epoch 35, Batch 500, Loss: 0.108\n",
            "Epoch 36, Batch 100, Loss: 0.071\n",
            "Epoch 36, Batch 200, Loss: 0.084\n",
            "Epoch 36, Batch 300, Loss: 0.076\n",
            "Epoch 36, Batch 400, Loss: 0.090\n",
            "Epoch 36, Batch 500, Loss: 0.090\n",
            "Epoch 37, Batch 100, Loss: 0.076\n",
            "Epoch 37, Batch 200, Loss: 0.087\n",
            "Epoch 37, Batch 300, Loss: 0.067\n",
            "Epoch 37, Batch 400, Loss: 0.087\n",
            "Epoch 37, Batch 500, Loss: 0.073\n",
            "Epoch 38, Batch 100, Loss: 0.079\n",
            "Epoch 38, Batch 200, Loss: 0.073\n",
            "Epoch 38, Batch 300, Loss: 0.087\n",
            "Epoch 38, Batch 400, Loss: 0.087\n",
            "Epoch 38, Batch 500, Loss: 0.077\n",
            "Epoch 39, Batch 100, Loss: 0.054\n",
            "Epoch 39, Batch 200, Loss: 0.086\n",
            "Epoch 39, Batch 300, Loss: 0.074\n",
            "Epoch 39, Batch 400, Loss: 0.076\n",
            "Epoch 39, Batch 500, Loss: 0.101\n",
            "Epoch 40, Batch 100, Loss: 0.057\n",
            "Epoch 40, Batch 200, Loss: 0.068\n",
            "Epoch 40, Batch 300, Loss: 0.080\n",
            "Epoch 40, Batch 400, Loss: 0.069\n",
            "Epoch 40, Batch 500, Loss: 0.076\n",
            "Epoch 41, Batch 100, Loss: 0.064\n",
            "Epoch 41, Batch 200, Loss: 0.082\n",
            "Epoch 41, Batch 300, Loss: 0.103\n",
            "Epoch 41, Batch 400, Loss: 0.082\n",
            "Epoch 41, Batch 500, Loss: 0.080\n",
            "Epoch 42, Batch 100, Loss: 0.055\n",
            "Epoch 42, Batch 200, Loss: 0.058\n",
            "Epoch 42, Batch 300, Loss: 0.093\n",
            "Epoch 42, Batch 400, Loss: 0.075\n",
            "Epoch 42, Batch 500, Loss: 0.084\n",
            "Epoch 43, Batch 100, Loss: 0.046\n",
            "Epoch 43, Batch 200, Loss: 0.076\n",
            "Epoch 43, Batch 300, Loss: 0.098\n",
            "Epoch 43, Batch 400, Loss: 0.084\n",
            "Epoch 43, Batch 500, Loss: 0.057\n",
            "Epoch 44, Batch 100, Loss: 0.063\n",
            "Epoch 44, Batch 200, Loss: 0.075\n",
            "Epoch 44, Batch 300, Loss: 0.080\n",
            "Epoch 44, Batch 400, Loss: 0.062\n",
            "Epoch 44, Batch 500, Loss: 0.082\n",
            "Epoch 45, Batch 100, Loss: 0.053\n",
            "Epoch 45, Batch 200, Loss: 0.061\n",
            "Epoch 45, Batch 300, Loss: 0.083\n",
            "Epoch 45, Batch 400, Loss: 0.060\n",
            "Epoch 45, Batch 500, Loss: 0.083\n",
            "Epoch 46, Batch 100, Loss: 0.052\n",
            "Epoch 46, Batch 200, Loss: 0.057\n",
            "Epoch 46, Batch 300, Loss: 0.071\n",
            "Epoch 46, Batch 400, Loss: 0.062\n",
            "Epoch 46, Batch 500, Loss: 0.068\n",
            "Epoch 47, Batch 100, Loss: 0.061\n",
            "Epoch 47, Batch 200, Loss: 0.050\n",
            "Epoch 47, Batch 300, Loss: 0.074\n",
            "Epoch 47, Batch 400, Loss: 0.064\n",
            "Epoch 47, Batch 500, Loss: 0.074\n",
            "Epoch 48, Batch 100, Loss: 0.054\n",
            "Epoch 48, Batch 200, Loss: 0.054\n",
            "Epoch 48, Batch 300, Loss: 0.068\n",
            "Epoch 48, Batch 400, Loss: 0.070\n",
            "Epoch 48, Batch 500, Loss: 0.081\n",
            "Epoch 49, Batch 100, Loss: 0.057\n",
            "Epoch 49, Batch 200, Loss: 0.066\n",
            "Epoch 49, Batch 300, Loss: 0.070\n",
            "Epoch 49, Batch 400, Loss: 0.058\n",
            "Epoch 49, Batch 500, Loss: 0.078\n",
            "Epoch 50, Batch 100, Loss: 0.035\n",
            "Epoch 50, Batch 200, Loss: 0.061\n",
            "Epoch 50, Batch 300, Loss: 0.051\n",
            "Epoch 50, Batch 400, Loss: 0.058\n",
            "Epoch 50, Batch 500, Loss: 0.075\n",
            "Epoch 51, Batch 100, Loss: 0.054\n",
            "Epoch 51, Batch 200, Loss: 0.067\n",
            "Epoch 51, Batch 300, Loss: 0.057\n",
            "Epoch 51, Batch 400, Loss: 0.055\n",
            "Epoch 51, Batch 500, Loss: 0.066\n",
            "Epoch 52, Batch 100, Loss: 0.046\n",
            "Epoch 52, Batch 200, Loss: 0.062\n",
            "Epoch 52, Batch 300, Loss: 0.056\n",
            "Epoch 52, Batch 400, Loss: 0.051\n",
            "Epoch 52, Batch 500, Loss: 0.082\n",
            "Epoch 53, Batch 100, Loss: 0.043\n",
            "Epoch 53, Batch 200, Loss: 0.043\n",
            "Epoch 53, Batch 300, Loss: 0.056\n",
            "Epoch 53, Batch 400, Loss: 0.064\n",
            "Epoch 53, Batch 500, Loss: 0.061\n",
            "Epoch 54, Batch 100, Loss: 0.060\n",
            "Epoch 54, Batch 200, Loss: 0.052\n",
            "Epoch 54, Batch 300, Loss: 0.047\n",
            "Epoch 54, Batch 400, Loss: 0.087\n",
            "Epoch 54, Batch 500, Loss: 0.058\n",
            "Epoch 55, Batch 100, Loss: 0.042\n",
            "Epoch 55, Batch 200, Loss: 0.057\n",
            "Epoch 55, Batch 300, Loss: 0.060\n",
            "Epoch 55, Batch 400, Loss: 0.062\n",
            "Epoch 55, Batch 500, Loss: 0.062\n",
            "Epoch 56, Batch 100, Loss: 0.046\n",
            "Epoch 56, Batch 200, Loss: 0.060\n",
            "Epoch 56, Batch 300, Loss: 0.043\n",
            "Epoch 56, Batch 400, Loss: 0.053\n",
            "Epoch 56, Batch 500, Loss: 0.046\n",
            "Epoch 57, Batch 100, Loss: 0.043\n",
            "Epoch 57, Batch 200, Loss: 0.078\n",
            "Epoch 57, Batch 300, Loss: 0.038\n",
            "Epoch 57, Batch 400, Loss: 0.081\n",
            "Epoch 57, Batch 500, Loss: 0.040\n",
            "Epoch 58, Batch 100, Loss: 0.042\n",
            "Epoch 58, Batch 200, Loss: 0.056\n",
            "Epoch 58, Batch 300, Loss: 0.051\n",
            "Epoch 58, Batch 400, Loss: 0.053\n",
            "Epoch 58, Batch 500, Loss: 0.059\n",
            "Epoch 59, Batch 100, Loss: 0.039\n",
            "Epoch 59, Batch 200, Loss: 0.050\n",
            "Epoch 59, Batch 300, Loss: 0.055\n",
            "Epoch 59, Batch 400, Loss: 0.061\n",
            "Epoch 59, Batch 500, Loss: 0.068\n",
            "Epoch 60, Batch 100, Loss: 0.044\n",
            "Epoch 60, Batch 200, Loss: 0.053\n",
            "Epoch 60, Batch 300, Loss: 0.041\n",
            "Epoch 60, Batch 400, Loss: 0.045\n",
            "Epoch 60, Batch 500, Loss: 0.044\n",
            "Epoch 61, Batch 100, Loss: 0.038\n",
            "Epoch 61, Batch 200, Loss: 0.043\n",
            "Epoch 61, Batch 300, Loss: 0.053\n",
            "Epoch 61, Batch 400, Loss: 0.062\n",
            "Epoch 61, Batch 500, Loss: 0.062\n",
            "Epoch 62, Batch 100, Loss: 0.032\n",
            "Epoch 62, Batch 200, Loss: 0.047\n",
            "Epoch 62, Batch 300, Loss: 0.067\n",
            "Epoch 62, Batch 400, Loss: 0.050\n",
            "Epoch 62, Batch 500, Loss: 0.045\n",
            "Epoch 63, Batch 100, Loss: 0.039\n",
            "Epoch 63, Batch 200, Loss: 0.043\n",
            "Epoch 63, Batch 300, Loss: 0.040\n",
            "Epoch 63, Batch 400, Loss: 0.042\n",
            "Epoch 63, Batch 500, Loss: 0.053\n",
            "Epoch 64, Batch 100, Loss: 0.038\n",
            "Epoch 64, Batch 200, Loss: 0.055\n",
            "Epoch 64, Batch 300, Loss: 0.054\n",
            "Epoch 64, Batch 400, Loss: 0.037\n",
            "Epoch 64, Batch 500, Loss: 0.044\n",
            "Epoch 65, Batch 100, Loss: 0.032\n",
            "Epoch 65, Batch 200, Loss: 0.024\n",
            "Epoch 65, Batch 300, Loss: 0.048\n",
            "Epoch 65, Batch 400, Loss: 0.054\n",
            "Epoch 65, Batch 500, Loss: 0.059\n",
            "Epoch 66, Batch 100, Loss: 0.056\n",
            "Epoch 66, Batch 200, Loss: 0.050\n",
            "Epoch 66, Batch 300, Loss: 0.051\n",
            "Epoch 66, Batch 400, Loss: 0.052\n",
            "Epoch 66, Batch 500, Loss: 0.058\n",
            "Epoch 67, Batch 100, Loss: 0.027\n",
            "Epoch 67, Batch 200, Loss: 0.047\n",
            "Epoch 67, Batch 300, Loss: 0.052\n",
            "Epoch 67, Batch 400, Loss: 0.049\n",
            "Epoch 67, Batch 500, Loss: 0.043\n",
            "Epoch 68, Batch 100, Loss: 0.041\n",
            "Epoch 68, Batch 200, Loss: 0.055\n",
            "Epoch 68, Batch 300, Loss: 0.038\n",
            "Epoch 68, Batch 400, Loss: 0.038\n",
            "Epoch 68, Batch 500, Loss: 0.054\n",
            "Epoch 69, Batch 100, Loss: 0.038\n",
            "Epoch 69, Batch 200, Loss: 0.045\n",
            "Epoch 69, Batch 300, Loss: 0.036\n",
            "Epoch 69, Batch 400, Loss: 0.055\n",
            "Epoch 69, Batch 500, Loss: 0.045\n",
            "Epoch 70, Batch 100, Loss: 0.030\n",
            "Epoch 70, Batch 200, Loss: 0.058\n",
            "Epoch 70, Batch 300, Loss: 0.062\n",
            "Epoch 70, Batch 400, Loss: 0.051\n",
            "Epoch 70, Batch 500, Loss: 0.044\n",
            "Epoch 71, Batch 100, Loss: 0.049\n",
            "Epoch 71, Batch 200, Loss: 0.038\n",
            "Epoch 71, Batch 300, Loss: 0.051\n",
            "Epoch 71, Batch 400, Loss: 0.040\n",
            "Epoch 71, Batch 500, Loss: 0.062\n",
            "Epoch 72, Batch 100, Loss: 0.032\n",
            "Epoch 72, Batch 200, Loss: 0.041\n",
            "Epoch 72, Batch 300, Loss: 0.039\n",
            "Epoch 72, Batch 400, Loss: 0.044\n",
            "Epoch 72, Batch 500, Loss: 0.054\n",
            "Epoch 73, Batch 100, Loss: 0.019\n",
            "Epoch 73, Batch 200, Loss: 0.047\n",
            "Epoch 73, Batch 300, Loss: 0.041\n",
            "Epoch 73, Batch 400, Loss: 0.039\n",
            "Epoch 73, Batch 500, Loss: 0.040\n",
            "Epoch 74, Batch 100, Loss: 0.032\n",
            "Epoch 74, Batch 200, Loss: 0.029\n",
            "Epoch 74, Batch 300, Loss: 0.034\n",
            "Epoch 74, Batch 400, Loss: 0.068\n",
            "Epoch 74, Batch 500, Loss: 0.037\n",
            "Epoch 75, Batch 100, Loss: 0.036\n",
            "Epoch 75, Batch 200, Loss: 0.050\n",
            "Epoch 75, Batch 300, Loss: 0.055\n",
            "Epoch 75, Batch 400, Loss: 0.040\n",
            "Epoch 75, Batch 500, Loss: 0.044\n",
            "Epoch 76, Batch 100, Loss: 0.039\n",
            "Epoch 76, Batch 200, Loss: 0.035\n",
            "Epoch 76, Batch 300, Loss: 0.046\n",
            "Epoch 76, Batch 400, Loss: 0.050\n",
            "Epoch 76, Batch 500, Loss: 0.043\n",
            "Epoch 77, Batch 100, Loss: 0.037\n",
            "Epoch 77, Batch 200, Loss: 0.039\n",
            "Epoch 77, Batch 300, Loss: 0.051\n",
            "Epoch 77, Batch 400, Loss: 0.026\n",
            "Epoch 77, Batch 500, Loss: 0.036\n",
            "Epoch 78, Batch 100, Loss: 0.031\n",
            "Epoch 78, Batch 200, Loss: 0.059\n",
            "Epoch 78, Batch 300, Loss: 0.031\n",
            "Epoch 78, Batch 400, Loss: 0.041\n",
            "Epoch 78, Batch 500, Loss: 0.032\n",
            "Epoch 79, Batch 100, Loss: 0.025\n",
            "Epoch 79, Batch 200, Loss: 0.049\n",
            "Epoch 79, Batch 300, Loss: 0.035\n",
            "Epoch 79, Batch 400, Loss: 0.035\n",
            "Epoch 79, Batch 500, Loss: 0.041\n",
            "Epoch 80, Batch 100, Loss: 0.030\n",
            "Epoch 80, Batch 200, Loss: 0.037\n",
            "Epoch 80, Batch 300, Loss: 0.024\n",
            "Epoch 80, Batch 400, Loss: 0.036\n",
            "Epoch 80, Batch 500, Loss: 0.053\n",
            "Epoch 81, Batch 100, Loss: 0.021\n",
            "Epoch 81, Batch 200, Loss: 0.040\n",
            "Epoch 81, Batch 300, Loss: 0.039\n",
            "Epoch 81, Batch 400, Loss: 0.062\n",
            "Epoch 81, Batch 500, Loss: 0.036\n",
            "Epoch 82, Batch 100, Loss: 0.030\n",
            "Epoch 82, Batch 200, Loss: 0.032\n",
            "Epoch 82, Batch 300, Loss: 0.028\n",
            "Epoch 82, Batch 400, Loss: 0.040\n",
            "Epoch 82, Batch 500, Loss: 0.045\n",
            "Epoch 83, Batch 100, Loss: 0.034\n",
            "Epoch 83, Batch 200, Loss: 0.036\n",
            "Epoch 83, Batch 300, Loss: 0.043\n",
            "Epoch 83, Batch 400, Loss: 0.045\n",
            "Epoch 83, Batch 500, Loss: 0.054\n",
            "Epoch 84, Batch 100, Loss: 0.019\n",
            "Epoch 84, Batch 200, Loss: 0.036\n",
            "Epoch 84, Batch 300, Loss: 0.060\n",
            "Epoch 84, Batch 400, Loss: 0.040\n",
            "Epoch 84, Batch 500, Loss: 0.035\n",
            "Epoch 85, Batch 100, Loss: 0.018\n",
            "Epoch 85, Batch 200, Loss: 0.037\n",
            "Epoch 85, Batch 300, Loss: 0.052\n",
            "Epoch 85, Batch 400, Loss: 0.020\n",
            "Epoch 85, Batch 500, Loss: 0.028\n",
            "Epoch 86, Batch 100, Loss: 0.034\n",
            "Epoch 86, Batch 200, Loss: 0.034\n",
            "Epoch 86, Batch 300, Loss: 0.046\n",
            "Epoch 86, Batch 400, Loss: 0.035\n",
            "Epoch 86, Batch 500, Loss: 0.041\n",
            "Epoch 87, Batch 100, Loss: 0.022\n",
            "Epoch 87, Batch 200, Loss: 0.048\n",
            "Epoch 87, Batch 300, Loss: 0.033\n",
            "Epoch 87, Batch 400, Loss: 0.043\n",
            "Epoch 87, Batch 500, Loss: 0.033\n",
            "Epoch 88, Batch 100, Loss: 0.029\n",
            "Epoch 88, Batch 200, Loss: 0.033\n",
            "Epoch 88, Batch 300, Loss: 0.035\n",
            "Epoch 88, Batch 400, Loss: 0.035\n",
            "Epoch 88, Batch 500, Loss: 0.038\n",
            "Epoch 89, Batch 100, Loss: 0.037\n",
            "Epoch 89, Batch 200, Loss: 0.034\n",
            "Epoch 89, Batch 300, Loss: 0.021\n",
            "Epoch 89, Batch 400, Loss: 0.048\n",
            "Epoch 89, Batch 500, Loss: 0.032\n",
            "Epoch 90, Batch 100, Loss: 0.041\n",
            "Epoch 90, Batch 200, Loss: 0.028\n",
            "Epoch 90, Batch 300, Loss: 0.027\n",
            "Epoch 90, Batch 400, Loss: 0.024\n",
            "Epoch 90, Batch 500, Loss: 0.050\n",
            "Epoch 91, Batch 100, Loss: 0.034\n",
            "Epoch 91, Batch 200, Loss: 0.028\n",
            "Epoch 91, Batch 300, Loss: 0.040\n",
            "Epoch 91, Batch 400, Loss: 0.047\n",
            "Epoch 91, Batch 500, Loss: 0.035\n",
            "Epoch 92, Batch 100, Loss: 0.027\n",
            "Epoch 92, Batch 200, Loss: 0.033\n",
            "Epoch 92, Batch 300, Loss: 0.024\n",
            "Epoch 92, Batch 400, Loss: 0.039\n",
            "Epoch 92, Batch 500, Loss: 0.035\n",
            "Epoch 93, Batch 100, Loss: 0.027\n",
            "Epoch 93, Batch 200, Loss: 0.032\n",
            "Epoch 93, Batch 300, Loss: 0.029\n",
            "Epoch 93, Batch 400, Loss: 0.035\n",
            "Epoch 93, Batch 500, Loss: 0.029\n",
            "Epoch 94, Batch 100, Loss: 0.035\n",
            "Epoch 94, Batch 200, Loss: 0.031\n",
            "Epoch 94, Batch 300, Loss: 0.032\n",
            "Epoch 94, Batch 400, Loss: 0.035\n",
            "Epoch 94, Batch 500, Loss: 0.033\n",
            "Epoch 95, Batch 100, Loss: 0.022\n",
            "Epoch 95, Batch 200, Loss: 0.045\n",
            "Epoch 95, Batch 300, Loss: 0.024\n",
            "Epoch 95, Batch 400, Loss: 0.040\n",
            "Epoch 95, Batch 500, Loss: 0.038\n",
            "Epoch 96, Batch 100, Loss: 0.024\n",
            "Epoch 96, Batch 200, Loss: 0.029\n",
            "Epoch 96, Batch 300, Loss: 0.039\n",
            "Epoch 96, Batch 400, Loss: 0.033\n",
            "Epoch 96, Batch 500, Loss: 0.031\n",
            "Epoch 97, Batch 100, Loss: 0.036\n",
            "Epoch 97, Batch 200, Loss: 0.036\n",
            "Epoch 97, Batch 300, Loss: 0.025\n",
            "Epoch 97, Batch 400, Loss: 0.028\n",
            "Epoch 97, Batch 500, Loss: 0.048\n",
            "Epoch 98, Batch 100, Loss: 0.018\n",
            "Epoch 98, Batch 200, Loss: 0.026\n",
            "Epoch 98, Batch 300, Loss: 0.037\n",
            "Epoch 98, Batch 400, Loss: 0.031\n",
            "Epoch 98, Batch 500, Loss: 0.029\n",
            "Epoch 99, Batch 100, Loss: 0.022\n",
            "Epoch 99, Batch 200, Loss: 0.025\n",
            "Epoch 99, Batch 300, Loss: 0.049\n",
            "Epoch 99, Batch 400, Loss: 0.068\n",
            "Epoch 99, Batch 500, Loss: 0.044\n",
            "Epoch 100, Batch 100, Loss: 0.034\n",
            "Epoch 100, Batch 200, Loss: 0.022\n",
            "Epoch 100, Batch 300, Loss: 0.025\n",
            "Epoch 100, Batch 400, Loss: 0.025\n",
            "Epoch 100, Batch 500, Loss: 0.031\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#05.TEST\n",
        "\n"
      ],
      "metadata": {
        "id": "HwUPEXwFh_BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 루프\n",
        "correct = 0\n",
        "total = 0\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "id": "qSxAagq-iBxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39db5a5e-a088-4d94-c630-5a27830953c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 72.77%\n"
          ]
        }
      ]
    }
  ]
}